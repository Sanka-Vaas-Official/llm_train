{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A model that takes in a puzzle-like reasoning-heavy question in English, and responds with a well-reasoned, step-by-step thought out response in Spanish.\"\n",
    "temperature = 0\n",
    "number_of_examples = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai tenacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import random\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "\n",
    "openai.api_key = \"YOUR KEY HERE\"\n",
    "\n",
    "N_RETRIES = 3\n",
    "\n",
    "@retry(stop=stop_after_attempt(N_RETRIES), wait=wait_exponential(multiplier=1, min=4, max=70))\n",
    "def generate_example(prompt, prev_examples, temperature=.5):\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"You are generating data which will be used to train a machine learning model.\\n\\nYou will be given a high-level description of the model we want to train, and from that, you will generate data samples, each with a prompt/response pair.\\n\\nYou will do so in this format:\\n```\\nprompt\\n-----------\\n$prompt_goes_here\\n-----------\\n\\nresponse\\n-----------\\n$response_goes_here\\n-----------\\n```\\n\\nOnly one prompt/response pair should be generated per turn.\\n\\nFor each turn, make the example slightly more complex than the last, while ensuring diversity.\\n\\nMake sure your samples are unique and diverse, yet high-quality and complex enough to train a well-performing model.\\n\\nHere is the type of model we want to train:\\n`{prompt}`\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    if len(prev_examples) > 0:\n",
    "        if len(prev_examples) > 8:\n",
    "            prev_examples = random.sample(prev_examples, 8)\n",
    "        for example in prev_examples:\n",
    "            messages.append({\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": example\n",
    "            })\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=1000,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message['content']\n",
    "\n",
    "# Generate examples\n",
    "prev_examples = []\n",
    "for i in range(number_of_examples):\n",
    "    print(f'Generating example {i}')\n",
    "    example = generate_example(prompt, prev_examples, temperature)\n",
    "    prev_examples.append(example)\n",
    "\n",
    "print(prev_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate system message\n",
    "def generate_system_message(prompt):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"You are a reasoning and problem-solving model trained to handle puzzle-like, logic-based questions in English. \"\n",
    "                    \"For each question, you will carefully break down the problem, step-by-step, analyzing the components and reasoning through the solution process. \"\n",
    "                    \"Once the problem is fully understood, you will provide a well-reasoned response in Spanish, ensuring that all logical deductions and steps are clearly explained. \"\n",
    "                    \"Your responses should be structured to show the entire thought process, ensuring clarity in both English and Spanish.\"\n",
    "                ),\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt.strip(),\n",
    "            }\n",
    "        ],\n",
    "        temperature=0.5,\n",
    "        max_tokens=500,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message['content']\n",
    "\n",
    "system_message = generate_system_message(prompt)\n",
    "\n",
    "print(f'The system message is: `{system_message}`. Feel free to re-run this cell if you want a better result.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize lists to store prompts and responses\n",
    "prompts = []\n",
    "responses = []\n",
    "\n",
    "# Parse out prompts and responses from examples\n",
    "for example in prev_examples:\n",
    "  try:\n",
    "    split_example = example.split('-----------')\n",
    "    prompts.append(split_example[1].strip())\n",
    "    responses.append(split_example[3].strip())\n",
    "  except:\n",
    "    pass\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'prompt': prompts,\n",
    "    'response': responses\n",
    "})\n",
    "\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "print('There are ' + str(len(df)) + ' successfully-generated examples.')\n",
    "\n",
    "# Initialize list to store training examples\n",
    "training_examples = []\n",
    "\n",
    "# Create examples in the GPT-3.5 fine-tuning format\n",
    "for index, row in df.iterrows():\n",
    "    training_example = {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_message.strip()},\n",
    "            {\"role\": \"user\", \"content\": row['prompt']},\n",
    "            {\"role\": \"assistant\", \"content\": row['response']}\n",
    "        ]\n",
    "    }\n",
    "    training_examples.append(training_example)\n",
    "\n",
    "# Save training examples to a .jsonl file\n",
    "with open('training_examples.jsonl', 'w') as f:\n",
    "    for example in training_examples:\n",
    "        f.write(json.dumps(example) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_id = openai.File.create(\n",
    "  file=open(\"/content/training_examples.jsonl\", \"rb\"),\n",
    "  purpose='fine-tune'\n",
    ").id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = openai.FineTuningJob.create(training_file=file_id, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "job_id = job.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.FineTuningJob.list_events(id=job_id, limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_pre_object = openai.FineTuningJob.retrieve(job_id)\n",
    "model_name = model_name_pre_object.fine_tuned_model\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "    model=model_name,\n",
    "    messages=[\n",
    "      {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_message,\n",
    "      },\n",
    "      {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": df['prompt'].sample().values[0],\n",
    "      }\n",
    "    ],\n",
    ")\n",
    "\n",
    "response.choices[0].message['content']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sanka_vir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
